{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline, ControlNetModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "from typing import Optional, Dict, Any, Union, List, Callable\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import autocast\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomStableDiffusion(StableDiffusionPipeline):\n",
    "    \"\"\"继承官方Pipeline并进行定制化扩展\"\"\"\n",
    "    \n",
    "    def add_extension(\n",
    "        self,\n",
    "        controlnet: Optional[ControlNetModel] = None,\n",
    "        precompiled_embeddings: Optional[Dict[str, torch.Tensor]] = None,\n",
    "        enable_xformers: bool = True,  # 新增xformers开关\n",
    "        enable_slicing: bool = False,    # 新增attention slicing开关\n",
    "        enable_vae_slicing: bool = False, # 新增VAE切片支持\n",
    "        enable_model_cpu_offload: bool = False, # 新增模型卸载支持\n",
    "    ):\n",
    "        # 显存优化配置\n",
    "        if enable_slicing:\n",
    "            self.enable_attention_slicing()\n",
    "        \n",
    "        if enable_vae_slicing:\n",
    "            self.enable_vae_slicing()\n",
    "            \n",
    "        if enable_model_cpu_offload:\n",
    "            self.enable_model_cpu_offload()\n",
    "            \n",
    "        try:\n",
    "            if enable_xformers:\n",
    "                self.enable_xformers_memory_efficient_attention()\n",
    "        except ImportError:\n",
    "            print(\"xformers未安装，跳过初始化。建议安装以获得更好性能：pip install xformers\")\n",
    "\n",
    "        # ControlNet扩展\n",
    "        if controlnet:\n",
    "            self.controlnet = controlnet.to(self.device)\n",
    "            # 在此添加ControlNet与UNet的融合逻辑\n",
    "        \n",
    "        # 预编译embedding初始化\n",
    "        self.precompiled_embeddings = precompiled_embeddings or {}\n",
    "\n",
    "    def _encode_prompt(\n",
    "        self,\n",
    "        prompt,\n",
    "        device,\n",
    "        num_images_per_prompt,\n",
    "        do_classifier_free_guidance,\n",
    "        negative_prompt=None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        lora_scale: Optional[float] = None,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        cache_key: Optional[str] = None,  # 新增缓存键参数\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"扩展文本编码支持预编译embedding\"\"\"\n",
    "        if cache_key and cache_key in self.precompiled_embeddings:\n",
    "            return self.precompiled_embeddings[cache_key]\n",
    "            \n",
    "        # 调用父类原始编码逻辑\n",
    "        prompt_embeds, negative_prompt_embeds = super().encode_prompt(\n",
    "            prompt,\n",
    "            device,\n",
    "            num_images_per_prompt,\n",
    "            do_classifier_free_guidance,\n",
    "            negative_prompt,\n",
    "            prompt_embeds,\n",
    "            negative_prompt_embeds,\n",
    "            lora_scale,\n",
    "            clip_skip,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        # 缓存计算结果\n",
    "        if cache_key:\n",
    "            self.precompiled_embeddings[cache_key] = (prompt_embeds, negative_prompt_embeds)\n",
    "            \n",
    "        return prompt_embeds, negative_prompt_embeds\n",
    "\n",
    "    def save_text_embeddings(self, cache_path: str):\n",
    "        \"\"\"保存预编译embedding\"\"\"\n",
    "        torch.save(self.precompiled_embeddings, cache_path)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        height: Optional[int] = None,\n",
    "        width: Optional[int] = None,\n",
    "        num_inference_steps: int = 28,\n",
    "        guidance_scale: float = 7.5,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        eta: float = 0.0,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        guidance_rescale: float = 0.0,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        latents: Optional[torch.Tensor] = None,\n",
    "        # 新增扩展参数\n",
    "        control_image: Optional[torch.Tensor] = None,\n",
    "        embedding_cache_key: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"扩展调用方法，兼容原生参数与扩展参数\"\"\"\n",
    "        \n",
    "        # 处理ControlNet输入\n",
    "        if hasattr(self, 'controlnet') and self.controlnet and control_image is not None:\n",
    "            control_image = control_image.to(self.device)\n",
    "            # 在此注入ControlNet处理逻辑\n",
    "            kwargs.update({\"controlnet_cond\": control_image})\n",
    "        \n",
    "        # 处理预编译embedding\n",
    "        if embedding_cache_key:\n",
    "            prompt_embeds, negative_prompt_embeds = self.precompiled_embeddings[embedding_cache_key]\n",
    "            prompt = None  # 必须清空原始prompt\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            # 调用父类实现\n",
    "            return super().__call__(\n",
    "                prompt=prompt,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                negative_prompt=negative_prompt,\n",
    "                num_images_per_prompt=num_images_per_prompt,\n",
    "                eta=eta,\n",
    "                generator=generator,\n",
    "                prompt_embeds=prompt_embeds,\n",
    "                negative_prompt_embeds=negative_prompt_embeds,\n",
    "                output_type=output_type,\n",
    "                return_dict=return_dict,\n",
    "                cross_attention_kwargs=cross_attention_kwargs,\n",
    "                guidance_rescale=guidance_rescale,\n",
    "                clip_skip=clip_skip,\n",
    "                latents=latents,\n",
    "                **kwargs\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:15<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 114514\n",
    "generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "\n",
    "# 使用半精度优化\n",
    "pipeline = CustomStableDiffusion.from_pretrained(\n",
    "    \"/workspace/sd_models/stable-diffusion-2-1-base\",\n",
    "    torch_dtype=torch.float32  \n",
    ").to(\"cuda\")\n",
    "\n",
    "# 启用所有优化\n",
    "pipeline.add_extension(\n",
    "    controlnet=None,\n",
    "    enable_xformers=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成LQ与文本嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 读取LQ文件\n",
    "from processor.image_processor import process_image\n",
    "lq_path = \"/workspace/datasets/SD_Rest/val/motion-blurry/LQ/GOPR0384_11_00_000001.png\"\n",
    "# lq_path = \"/workspace/datasets/SD_Rest/val/motion-blurry/GT/GOPR0384_11_00_000001.png\"\n",
    "lq_image = process_image(lq_path).to('cuda')\n",
    "\n",
    "# save the tensor image\n",
    "# convert tensor to PIL image\n",
    "input_image = pipeline.image_processor.postprocess(lq_image, output_type=\"pil\", do_denormalize=[True] * lq_image.shape[0])[0]\n",
    "input_image.save(\"input_image.jpg\")\n",
    "\n",
    "# 生成LQ的VAE嵌入\n",
    "vae_embedding = pipeline.vae.encode(\n",
    "    lq_image,\n",
    ")\n",
    "begin_latents = vae_embedding[0].sample() * pipeline.vae.config.scaling_factor\n",
    "\n",
    "begin_image = pipeline.vae.decode(begin_latents / pipeline.vae.config.scaling_factor, return_dict=False, generator=generator)[\n",
    "                0\n",
    "            ]\n",
    "begin_image = pipeline.image_processor.postprocess(begin_image.detach(), output_type=\"pil\", do_denormalize=[True] * begin_image.shape[0])[0]\n",
    "begin_image = begin_image.save(\"begin_image.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:06<00:00,  9.88s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 生成时进一步优化\n",
    "guidance_scale = 5\n",
    "image = pipeline(\n",
    "    \"people walking on a city street with a lot of buildings\",\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=guidance_scale,\n",
    "    # latents=begin_latents,\n",
    "    generator=generator,\n",
    ")\n",
    "# 保存生成结果\n",
    "image.images[0].save(f\"generated_image_{guidance_scale}.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
